{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "var code_shown = true;\n",
    "document.addEventListener(\"DOMContentLoaded\",function(){\n",
    "        code_toggle();\n",
    "});\n",
    "  function code_toggle() {\n",
    "    var cells = document.getElementsByClassName('input');\n",
    "    var l = cells.length;\n",
    "    if (code_shown){\n",
    "        for (i= 0; i < l; i++) {\n",
    "            cells[i].style.display = 'none';\n",
    "        }\n",
    "        document.getElementById('toggleCodeBtn').value = 'Show code'\n",
    "      } else {\n",
    "          console.log('2');\n",
    "        for (i= 0; i < l; i++) {\n",
    "            cells[i].style.display = 'block';\n",
    "        }\n",
    "        document.getElementById('toggleCodeBtn').value = 'Hide code';\n",
    "      }\n",
    "        code_shown = !code_shown;\n",
    " }\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\" id=\"toogleCode\"><input type=\"submit\" class=\"btn btn-danger\" id=\"toggleCodeBtn\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows different analyses attempted to analyse the content of the Facebook pages.\n",
    "* Analyse content from the posts\n",
    "    * What different is there between PUK and not PUK\n",
    "* Find most \"succesful\" posts\n",
    "    * at what time?\n",
    "    * what content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import json\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import TokenSearcher\n",
    "import string\n",
    "import io\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1- Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We start with removing the punctuation, and separate word for word to remove the common words in the English language. Also replace \"parkinson's uk\" as one word to be able to separate it from \"parkinson's\". Correct some obvious mispelling in common words that appeared many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['https','http','org',u'“',u'’',u'–','www']\n",
    "dic_replace = {\"parkinson u'\\u2019' s uk \":'parkinsonsuk ',\"parkinson u'\\u2019' s\":'parkinsons',\n",
    "               \"parkinson's uk \":'parkinsonsuk ',\"parkinson's\":'parkinsons',\n",
    "               'carers ':'carer ','thank you ':'thanks','weeks ':'week',\n",
    "               'treatmen ':'treatment','sympto ':'symptoms','symptom ':'symptoms',\n",
    "               'dads ':'dad', 'mums':'mum'}\n",
    " \n",
    "def tokenize(s):\n",
    "    return nltk.word_tokenize(s)\n",
    " \n",
    "def preprocess(s):\n",
    "    s = s.lower()\n",
    "    for w in dic_replace:\n",
    "        s = s.replace(w,dic_replace[w])\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "def lightclean(s):\n",
    "    s.lower()\n",
    "    for w in dic_replace:\n",
    "        s = s.replace(w,dic_replace[w])\n",
    "    for p in punctuation:\n",
    "        s = s.replace(p,'')\n",
    "    return s\n",
    "\n",
    "def cleantext(fname,analysis_name):\n",
    "    error = 0\n",
    "    with open(fname, 'r') as f:\n",
    "        count_stop = Counter()\n",
    "        count_bigram = Counter()\n",
    "        for line in f:\n",
    "            posts = json.loads('{}'.format(line))\n",
    "            for post in posts:\n",
    "                try:\n",
    "                    terms_stop = [term for term in preprocess(post['content']) \n",
    "                                  if term not in stop]\n",
    "                    terms_bigram = bigrams(terms_stop)\n",
    "                    terms = [term for term in preprocess(post['content'])\n",
    "                             if term not in stop and len(term) != 1]\n",
    "                except:\n",
    "                    error += 1\n",
    "                count_stop.update(terms_stop)\n",
    "                count_bigram.update(terms_bigram)\n",
    "\n",
    "    nElements = 50\n",
    "    with open('bigrams_'+analysis_name+'.txt', 'w') as f:\n",
    "        f.write(str(count_bigram.most_common(nElements)))\n",
    "    word_freq = count_stop.most_common(nElements)\n",
    "    # Export the word frequency to json\n",
    "    with io.open('wordfreq_'+analysis_name+'.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(unicode(json.dumps(word_freq, ensure_ascii=False, encoding='utf8')))\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    "\n",
    "There are three datasets here: \n",
    "* all the posts, \n",
    "* all the posts from Parkinson's UK (PUK), \n",
    "* and all the posts not from Parkinson's UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'posts.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-84cb80545322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleantext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'posts.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-97e2736515c8>\u001b[0m in \u001b[0;36mcleantext\u001b[0;34m(fname, analysis_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleantext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalysis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mcount_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcount_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'posts.json'"
     ]
    }
   ],
   "source": [
    "cleantext('posts.json','all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleantext('posts_puk.json','puk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleantext('posts_notpuk.json','notpuk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare word used: PUK and not PUK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that Parkinson's UK (PUK) and their readers (not Puk) will have used different words in their text. This reflects their difference of interest but also of terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('posts_puk.json',encoding='utf-8') as f_puk, io.open('posts_notpuk.json',encoding='utf-8') as f_notpuk:\n",
    "    posts_puk = json.loads(f_puk.read(), encoding='utf8')\n",
    "    posts_notpuk = json.loads(f_notpuk.read(), encoding='utf8')\n",
    "print 'PUK wrote', len(posts_puk), 'posts'\n",
    "print 'Not PUK wrote', len(posts_notpuk), 'posts.'\n",
    "authors = []\n",
    "content_puk = []\n",
    "content_notpuk = []\n",
    "for i in range(len(posts_notpuk)):\n",
    "    authors.append(posts_notpuk[i]['person_hash_id'])\n",
    "    content_notpuk.append(lightclean(posts_notpuk[i]['content']))\n",
    "for i in range(len(posts_puk)):\n",
    "    content_puk.append(lightclean(posts_puk[i]['content']))\n",
    "authors = set(authors)\n",
    "print 'Not PUK posts were written by', len(authors), 'authors. So', round(len(posts_notpuk)/len(authors),1), 'posts per authors'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the data to make a bar chart of both 50 most common words in PUK and not PUK posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('wordfreq_puk.json', 'r') as fpuk, open('wordfreq_notpuk.json', 'r') as fnotpuk:\n",
    "    words_puk = json.load(fpuk)\n",
    "    words_notpuk = json.load(fnotpuk)\n",
    "freqwords_all = []\n",
    "for word in words_puk:\n",
    "    freqwords_all.append(word[0])\n",
    "for word in words_notpuk:\n",
    "    freqwords_all.append(word[0])\n",
    "# contains the list of 50 freq words between PUK and not PUK\n",
    "freqwords_all = list(set(freqwords_all)) \n",
    "# prepare data for the bar chart\n",
    "df_puk = pd.DataFrame(words_puk,columns=['word','PUK'])\n",
    "df_notpuk = pd.DataFrame(words_notpuk,columns=['word','NotPUK'])\n",
    "df = pd.merge(df_puk,df_notpuk,on='word',how='outer')\n",
    "df['diff'] = (df['PUK'] - df['NotPUK']).fillna(0)\n",
    "df = df.sort_values(['diff'])\n",
    "df_plot = pd.melt(df,id_vars=['word'],value_vars=['PUK','NotPUK'])\n",
    "df_plot = df_plot.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a chart of the 50 most used words by Parkinson's UK and their readers. The following barchart shows words grouped together, one bar per writer type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(111)\n",
    "space = 0.3\n",
    "authorcat = np.unique(df_plot[:,1])\n",
    "freqwords = np.unique(df_plot[:,0])\n",
    "n = len(authorcat)\n",
    "width = (1 - space) / (len(authorcat))\n",
    "colors = ['b','r']\n",
    "for i,word in enumerate(authorcat):\n",
    "    allwords = df_plot[df_plot[:,1] == word][:,0]\n",
    "    indices = range(1, len(freqwords)+1)\n",
    "    vals = df_plot[df_plot[:,1] == word][:,2].astype(np.float)\n",
    "    pos = [j - (1 - space) / 2. + i * width for j in range(1,len(freqwords)+1)]\n",
    "    ax.bar(pos, vals, width=width, label=word, \n",
    "               color = colors[i])\n",
    "ax.set_xticks(indices)\n",
    "ax.set_xticklabels(allwords)\n",
    "plt.setp(plt.xticks()[1], rotation=90)\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlabel(\"Words\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], loc='upper left')\n",
    "plt.savefig('Word_Frequency_pukornot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick analysis\n",
    "The words with only a red bar in are only frequent in Parkinson's UK posts, while the blue bar only are only frequent in readers of Parkinson's UK. \n",
    "* Readers posting on Parkinson's UK page express themselves differently: they use polite words such as 'Hi', 'tanks', or 'please'. \n",
    "* They refer to the Parkinson's condition as a 'disease' or 'pd' (for Parkinson's disease), while Parkinson's UK use the word 'condition'. \n",
    "* Parkibson's UK only speaks frequently about 'diagnosis', while readers speak of 'diagnosed'\n",
    "* Regarding their use of indefinite pronouns, Parkinson's UK uses most frequently 'something' (along with the word 'things') while readers of Parkinson's UK uses 'anyone'.\n",
    "* Both use similarly 'raise', 'support', 'awareness'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We define four clusters:\n",
    "* The words frequent only in PUK's text\n",
    "* The words frequent only in not PUK's text\n",
    "* The words frequent for both, more often in PUK\n",
    "* The words frequent for both, more often in not PUK\n",
    "\n",
    "We first look at all the text from either author type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pukonly = df[df['NotPUK'].isnull()]\n",
    "df_notpukonly = df[df['PUK'].isnull()]\n",
    "df_morepuk = df.ix[(~df['NotPUK'].isnull() | ~df['PUK'].isnull())]#&df['PUK']>df['NotPUK']]\n",
    "df_morenotpuk = df.ix[(~df['NotPUK'].isnull() | ~df['PUK'].isnull())]#&df['PUK']<df['NotPUK']]\n",
    "print '* Only frequent in PUKs posts:'\n",
    "print ', '.join(str(x) for x in df_pukonly['word'].values)\n",
    "print '* Only frequent in not PUKs posts:'\n",
    "print ', '.join(str(x) for x in df_notpukonly['word'].values)\n",
    "print '* Frequent words in both, more frequent for PUK:'\n",
    "print ', '.join(str(x) for x in df_morepuk['word'].values)\n",
    "print '* Frequent words in both, more frequent for not PUK:'\n",
    "print ', '.join(str(x) for x in df_morenotpuk['word'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent words per author, per category\n",
    "Our first approximation overcount some words, when they are repeated by the same person. Therefore here we count the number of times it is mentioned by a new person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('posts_puk.json',encoding='utf-8') as f:\n",
    "    puk = json.load(f)\n",
    "with io.open('posts_notpuk.json') as f:\n",
    "    notpuk = json.load(f)\n",
    "\n",
    "comp = []\n",
    "def count_s(s,data):\n",
    "    count = 0\n",
    "    if data == puk:\n",
    "        for i in range(len(data)):\n",
    "            if s in data[i]['content']:\n",
    "                count += 1\n",
    "        comp.append([\"Parkinson's UK\",s,count,round(100*count/len(data),1)])\n",
    "    else:\n",
    "        authorsaidit = []\n",
    "        for i in range(len(data)):\n",
    "            if s in data[i]['content'] :\n",
    "                if data[i]['person_hash_id'] not in authorsaidit:\n",
    "                    authorsaidit.append(data[i]['person_hash_id'])\n",
    "                    count += 1\n",
    "        comp.append([\"Not Parkinson's UK\",s,count,round(100*count/len(data),1)])\n",
    "    return comp\n",
    "\n",
    "for w in freqwords_all:\n",
    "    count_s(w,puk)\n",
    "    count_s(w,notpuk)\n",
    "df_comp_all = pd.DataFrame(comp,columns=['AuthorType','Word','NbpostsAuthors','Percentage'])\n",
    "    \n",
    "\n",
    "df_scatter = df_comp_all.copy()\n",
    "df_scatter = df_scatter[['AuthorType','Word','Percentage']].set_index(['AuthorType','Word'],append=True)\n",
    "df_scatter = df_scatter.unstack('AuthorType')\n",
    "df_scatter = df_scatter.stack(0)\n",
    "df_scatter = df_scatter.reset_index().drop(['level_0','level_2'],axis=1)\n",
    "df_scatter[\"Not Parkinson's UK\"] = df_scatter[\"Not Parkinson's UK\"].fillna(method='bfill')\n",
    "df_scatter[\"Parkinson's UK\"] = df_scatter[\"Parkinson's UK\"].fillna(method='ffill')\n",
    "df_scatter = df_scatter.drop_duplicates('Word')\n",
    "\n",
    "#print df_scatter\n",
    "#df_comp_puk = df_comp[['Word','NbpostsAuthors']].ix[df_comp['AuthorType']==\"Parkinson's UK\"]\n",
    "#df_comp_notpuk = df_comp[['Word','NbpostsAuthors']].ix[df_comp['AuthorType']==\"Not Parkinson's UK\"]\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg') # I think this was to solve a problem but it didn't work\n",
    "\n",
    "#df_scatter.plot.scatter(x=\"Parkinson's UK\", y=\"Not Parkinson's UK\")\n",
    "# set up figure and ax\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# the scatter plot:\n",
    "labels = df_scatter['Word']\n",
    "for label, x, y in zip(labels, df_scatter[\"Parkinson's UK\"],df_scatter[\"Not Parkinson's UK\"]):\n",
    "    plt.annotate(\n",
    "        label, \n",
    "        xy = (x, y), xytext = (10, 50),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.1', fc = 'white', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "\n",
    "x = df_scatter[\"Parkinson's UK\"]\n",
    "y = df_scatter[\"Not Parkinson's UK\"]\n",
    "fit = np.polyfit(x, y, deg=1)\n",
    "ax.plot(x, fit[0] * x + fit[1], 'g--')\n",
    "ax.plot([0,60],[0,60], 'r--', label='Random guess')\n",
    "ax.scatter(df_scatter[\"Parkinson's UK\"], df_scatter[\"Not Parkinson's UK\"], c='blue', s = 1)\n",
    "\n",
    "from bokeh.charts import Scatter, show\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "from bokeh.sampledata.iris import flowers as data\n",
    "\n",
    "scatter = Scatter(data, x='petal_length', y='petal_width',\n",
    "                  color='species', marker='species',\n",
    "                  title='Iris Dataset Color and Marker by Species',\n",
    "                  legend=True)\n",
    "\n",
    "#output_file(\"iris_simple.html\", title=\"iris_simple.py example\")\n",
    "\n",
    "show(scatter,notebook_handle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at a list of words which seem to have a different frequency in both texts:\n",
    "\n",
    "* 'dad' vs 'mum'\n",
    "* 'disease','condition'\n",
    "* 'help','support'\n",
    "* 'diagnosis', 'diagnosed'\n",
    "* 'research','money'\n",
    "* 'family','friends'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comp = []\n",
    "listoflistwords = [['dad','mum'],['disease','condition'],['help','support'],['diagnosis', \n",
    "             'diagnosed'],['research','money'],['family','friends']]\n",
    "listwords = ['dad','mum','disease','condition','help','support','diagnosis', \n",
    "             'diagnosed','research','money','family','friends']\n",
    "def barplotdata(listoflistwords,plotnb):\n",
    "    w2 = listoflistwords[plotnb-1]\n",
    "    for w in w2:\n",
    "        count_s(w,puk)\n",
    "        count_s(w,notpuk)\n",
    "    df_comp = pd.DataFrame(comp,columns=['AuthorType','Word','NbpostsAuthors','Percentage'])\n",
    "    return df_comp[['Percentage','Word']].ix[(df_comp['AuthorType']==\"Parkinson's UK\")], df_comp[['Percentage','Word']].ix[(df_comp['AuthorType']==\"Not Parkinson's UK\")]\n",
    "\n",
    "\n",
    "def create_subplot(nbplot,ax=None):\n",
    "    plot1puk, plot1notpuk = barplotdata(listoflistwords,nbplot)\n",
    "    plota = plot1puk.plot(kind='bar', color='red', ax=ax, position=0, width=0.25)\n",
    "    plotb = plot1notpuk.plot(kind='bar', color='blue', ax=ax, position=1, width=0.25)\n",
    "    return plota, plotb\n",
    "\n",
    "# make figure with subplots\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3,sharey=True, figsize=(10,5))\n",
    "\n",
    "# Plots\n",
    "create_subplot(1,ax1)\n",
    "ax1.set_xticklabels(listoflistwords[0], minor=False,rotation=0)\n",
    "comp = []\n",
    "create_subplot(2,ax2)\n",
    "ax2.set_xticklabels(listoflistwords[1], minor=False,rotation=0)\n",
    "comp = []\n",
    "create_subplot(3,ax3)\n",
    "ax3.set_xticklabels(listoflistwords[2], minor=False,rotation=0)\n",
    "comp = []\n",
    "create_subplot(4,ax4)\n",
    "ax4.set_xticklabels(listoflistwords[3], minor=False,rotation=0)\n",
    "comp = []\n",
    "create_subplot(5,ax5)\n",
    "ax5.set_xticklabels(listoflistwords[4], minor=False,rotation=0)\n",
    "comp = []\n",
    "create_subplot(6,ax6)\n",
    "ax6.set_xticklabels(listoflistwords[5], minor=False,rotation=0)\n",
    "\n",
    "ax1.legend([\"Parkinson's UK\",\"Not Parkinson's UK\"],fancybox=True,loc='upper left')\n",
    "ax2.legend().remove()\n",
    "ax3.legend().remove()\n",
    "ax4.legend().remove()\n",
    "ax5.legend().remove()\n",
    "ax6.legend().remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Context\n",
    "Let's look now into the context in which these words are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we need to use the tokenisation from NLTK\n",
    "import nltk.collocations\n",
    "import nltk.corpus\n",
    "import collections\n",
    "from nltk import word_tokenize, FreqDist\n",
    "#for x in content_puk:\n",
    "#    print x\n",
    "#[print str(x) for x in content_puk]\n",
    "text_puk = ' | '.join(x.lower() for x in content_puk)\n",
    "text_notpuk = ' | '.join(x.lower() for x in content_notpuk)\n",
    "textnltk_puk = nltk.Text(word_tokenize(text_puk))\n",
    "textnltk_notpuk = nltk.Text(word_tokenize(text_notpuk))\n",
    "def find_unique_exp(text,exp):\n",
    "    uniqu = []\n",
    "    match_tokens = TokenSearcher(text).findall(exp)\n",
    "    for x in match_tokens:\n",
    "        uniqu.append(' '.join(x))\n",
    "    return list(set(uniqu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's first look a the context in which some words appear. \n",
    "Puk and Not PUK talked about dad and mum at a different frequency. Let's see in which context these appear.\n",
    "#For instance, puk and not puk talked with a different frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Unique expressions of PUK for the female parent: '\n",
    "print find_unique_exp(textnltk_puk,r\"<.*> <.*> <mum> | <.*> <.*> <mums>| <.*> <.*> <mother>| <.*> <.*> <mothers>\")\n",
    "print '** and for the male parent'\n",
    "print find_unique_exp(textnltk_puk,r\"<.*> <.*> <dad> | <.*> <.*> <dads>| <.*> <.*> <father>| <.*> <.*> <father>\")\n",
    "print '------------------'\n",
    "print 'Unique expressions of Not PUK for the female parent: '\n",
    "print find_unique_exp(textnltk_notpuk,r\"<.*> <.*> <mum> | <.*> <.*> <mums>| <.*> <.*> <mother>| <.*> <.*> <mothers>\")\n",
    "print '** and for the male parent'\n",
    "print find_unique_exp(textnltk_notpuk,r\"<.*> <.*> <dad> | <.*> <.*> <dads>| <.*> <.*> <father>| <.*> <.*> <father>\")\n",
    "# maybe could look into the occurences of 'my dad' vs 'my mum' = are they speaking in general about a parent or \n",
    "#about their parent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expressions that contain 'Parkinson's\n",
    "print '* PUK: Find 3-word expression ending with Parkinson:'\n",
    "puk_3words_parkinson = find_unique_exp(textnltk_puk,r\"<.*> <.*> <parkinson> | <.*> <.*> <parkinsons>\")\n",
    "#print puk_3words_parkinson\n",
    "print '* Not PUK: Find 3-word expression ending with Parkinson:'\n",
    "notpuk_3words_parkinson =  find_unique_exp(textnltk_notpuk,r\"<.*> <.*> <parkinson> | <.*> <.*> <parkinsons>\")\n",
    "print '-------'\n",
    "print '* PUK: Find 2-word expression with money:'\n",
    "print find_unique_exp(textnltk_puk,r\"<.*> <money>\")\n",
    "print '* Not PUK: Find 2-word expression with money:'\n",
    "print find_unique_exp(textnltk_notpuk,r\"<.*> <money>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '-------'\n",
    "print '* Find words that often appear together:'\n",
    "#print textnltk_puk.collocations()\n",
    "print '-------'\n",
    "print textnltk_puk.concordance('mum')\n",
    "print textnltk_notpuk.concordance('mum')\n",
    "print '-------'\n",
    "#print textnltk_puk.concordance('raise')\n",
    "#print textnltk_notpuk.concordance('raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We could look into the context in which 'challenge' appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Money seems to be more often mentioned by not puk\n",
    "# what about the 'raise money' vs 'raise awarennes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "bgm    = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(textnltk_puk)\n",
    "finder_notpuk = nltk.collocations.BigramCollocationFinder.from_words(textnltk_notpuk)\n",
    "scored = finder.score_ngrams(bgm.likelihood_ratio)\n",
    "scored_notpuk = finder_notpuk.score_ngrams(bgm.likelihood_ratio)\n",
    "\n",
    "# Group bigrams by first word in bigram.                                        \n",
    "prefix_keys = collections.defaultdict(list)\n",
    "for key, scores in scored:\n",
    "   prefix_keys[key[0]].append((key[1], scores))\n",
    "\n",
    "# Sort keyed bigrams by strongest association.                                  \n",
    "for key in prefix_keys:\n",
    "   prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "print 'parkinsons', prefix_keys['parkinson'][:5]\n",
    "print 'diagnosed', prefix_keys['diagnosed'][:5]\n",
    "print 'affected', prefix_keys['affected'][:5]\n",
    "print 'raise', prefix_keys['raise'][:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's look at how efficient the different posts are = which one give more reactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to be talking about raising money, research, awareness, ..? maybe need first to read all the posts. Argh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How often do people post?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
